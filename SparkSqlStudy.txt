Spark Sql study

发展过程
hive => hive on spark => spark sql

RDD TO DataFrame(DataSet)

	reflection, 数据格式已知
	
	programmatically, 数据格式只能在程序运行时获得
	

DataFrame 和 RDD的区别


数据的load和save
	数据格式parquet, 通用的面向列的大数据存储格式

parquet文件操作
	Loading Data Programmatically编程方式加载数据
	Partition Discovery分区推断
	Schema Merging元数据合并
	
数据源之JSON
数据源之JDBC
数据源之Hive
	table-hive环境搭建
	table-spark环境集成
		spark-sql --jars /home/hadoop/hive2.1.1/lib/mysql-connector-java-6.0.6.jar
	
	使用
	LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
	
	drop table people;
	create table people (id INT, name STRING, address STRING,age INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
	load data local inpath '/home/hadoop/people.txt' OVERWRITE into table people;

	
	
	
数据源之HBase
	HBase环境搭建
		先安装zookeeper
		
		ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
			at org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2379)

		先启动regionserver，在启动HMaster。
		在regionServer上./hbase-daemon.sh start regionserver
		在master上执行：./bin/hbase-daemon.sh start master
		
		添加maven依赖
			hbase-client
			hbase-server
			
		spark1.6.3时，最好使用scala2.10.x开发

